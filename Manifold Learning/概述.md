# 流形学习概述

## 数据降维

在机器学习算法中，数据降维算法是一个大家族，既有有监督学习的版本，也有无监督学习的版本；既有线性的降维算法，也有非线性的降维算法。

最经典的数据降维算法要数PCA（主成分分析），这是一种线性降维算法，而且是无监督的，它通过线性变换将样本投影到低维空间中：

$$y = Wx$$

其中，$x$是输入向量，为$n$为向量，$W$是$m$行$n$列的投影矩阵，将$x$左乘它，可以得到一个$m$维的结果向量$y$。一般情况下，$m$远小于$n$，这样就将一个向量变换成另外一个更低维的向量，从而完成数据降维。PCA的矩阵$W$是通过样本学习得到的，其依据是最小化重构误差。

PCA是一种线性降维技术，对于非线性数据具有局限性，而在实际应用中很多时候数据是非线性的。此时可以采用非线性降维技术，它们都通过一个非线性的映射函数将输入向量x映射成一个更低维的向量y：

$$y=\phi(x)$$

问题的关键是这个非线性映射函数如何得到，一般来说，它要使得数据降维之后保持之前的某些结构信息。非线性降维算法的典型代表有核PCA（KPCA，核主成分分析），神经网络（如自动编码器），流形学习等。

## 流形

流形（manifold）是几何中的一个概念，它是高维空间中的几何结构，即空间中的点构成的集合。可以简单的将流形理解成二维空间的曲线，三维空间的曲面在更高维空间的推广。

## 流形学习

很多应用问题的数据在高维空间中的分布具有某种几何形状，即集中在某个低维的流形附近。对于前面所说的32x32的手写数字图像，数字7的图像在1024维空间中应该聚集在某一个形状的几何体周围（如带状区域，球面），其他的类别也是如此。

流形学习（manifold learning）**假设数据在高维空间的分布位于某一更低维的流形上**，基于这个假设来进行数据的分析。对于降维，要保证降维之后的数据同样**满足与高维空间流形有关的几何约束关系。**除此之外，流形学习还可以用实现聚类，分类以及回归算法。

假设有一个$N$维空间中的流形$M$，即$M$为$N$维欧氏空间的一个真子集：

$$M \subset N$$

流形学习降维算法要实现的是如下映射：

$$M \rightarrow \mathbb{R}^n$$

即将N维空间中流形M上的点映射为n维空间中的点。下面介绍几种典型的流形降维算法，包括局部线性映射，拉普拉斯特征映射，局部保持投影，等距映射。

### 局部线性嵌入

局部线性嵌入（简称LLE）的核心思想是每个样本点都可以由与它相邻的多个点的线性组合（体现了局部线性）来近似重构，这相当于用分段的线性面片近似代替复杂的几何形状，样本投影到低维空间之后要保持这种线性重构关系，即有相同的重构系数。

假设数据集由$l$个$D$维向量 $x_{i}$ 组成，它们分布在D维空间中的一个流形附近。每个数据点和它的邻居位于或者接近于流形的一个局部线性片段（平面，体现了线性，类似于微积分中的以直代曲的思想）上，即可以用邻居点的线性组合来重构，组合系数刻画了局部面片的几何特性：

$$x_{i} \sim \sum\limits_{i}w_{ij}x_j$$

权重$w_{ij}x_j$为第$j$个数据点对第$i$个点的组合权重，这些点的线性组合用来近似重构数据点$i$。权重系数通过最小化下面的重构误差确定：

$$\min\limits_{w_{ij}} \sum\limits_{i=1}^l||x_{i}-\sum\limits_{j=1}^lw_{ij}x_j||^2$$

在这里还加上了两个约束条件：每个点只由它的邻居来重构，如果 $x_{j}$ 不在 $x_{i}$ 的邻居集合里则权重值为0，这体现了局部性。另外限定权重矩阵的每一行元素之和为1，即：

$$\sum\limits_j w_{ij}=1$$

这是一个带约束的优化问题，求解该问题可以得到权重系数。这一问题和主成分分析要求解的问题类似。可以证明，这个权重值对平移、旋转、缩放等几何变换具有不变性。

假设算法将向量从D维空间的x映射为d维空间的y。每个点在d维空间中的坐标由下面的最优化问题确定：

$$\min\limits_{y_i} \sum\limits_{i=1}^l||y_i-\sum\limits_{j=1}^lw_{ij}y_j||^2$$

这里的权重和上一个优化问题的值相同，在前面已经得到。优化的目标是 $y_{i}$ ，这个优化问题等价于求解稀疏矩阵的特征值问题。得到$y$之后，即完成了从D维空间到d维空间的非线性降维。

### 拉普拉斯特征映射

拉普拉斯特征映射（简称LE）是基于图论的方法。它从样本点构造带权重的图，然后计算图的拉普拉斯矩，对该矩阵进行特征值分解得到投影变换矩阵。

假设有一批样本点 $x_{1},\cdots,x_{k}$ ，它们是 $R^{l}$ 空间的向量，降维的目标是将它们变换为更低维的 $R^{m}$ 空间中的向量 $y_{1},\cdots,y_{k}$ ，其中$m$<<$l$。在这里假设 $x_{1},...,x_{k}\in M$ ，其中$M$为嵌入$R^{l}$空间中的一个流形。

算法为样本点构造加权图，图的节点是每一个样本点，边为每个节点与它的邻居节点之间的相似度，每个节点只和它的邻居有连接关系。

算法的第一步是构造图的邻接关系。如果样本点 $x_{i}$ 和样本点 $x_{j}$ 的距离很近，则为图的节点$i$和节点$j$建立一条边。判断两个样本点是否解接近的方法有两种。第一种是计算二者的欧氏距离，如果距离小于某一值 $\varepsilon$ 则认为两个样本很接近。

第二步是计算边的权重，在这里也有两种选择。第一种方法为如果节点i和节点j是联通的，则它们之间的边的权重为：$w_{ij}=\exp(-\frac{||x_i-x_j||2}{t})$，否则$w_{ij}=0$。其中t是一个人工设定的大于0的实数。第二种方式是如果节点i和节点j是联通的则它们之间的边的权重为1，否则为0。

第三步是特征映射。假设构造的图是联通的，即任何两个节点之间都有路径可达，如果不联通，则算法分别作用于每个联通分量上。根据前面构造的图计算它的拉普拉斯矩阵，然后求解如下广义特征值和特征向量问题：

$$Lf=\lambda Df$$

由于是实对称矩阵半正定矩阵，因此特征值非负。假设 $f_{0},...,f_{k-1}$ 是这个广义特征值问题的解，它们按照特征值的大小升序排列，去掉值为0的特征值 $\lambda_{0}$ ，用剩下部分特征向量为行来构造投影矩阵，将向量投影到以它们为基的空间中。

## 推荐阅读

https://blog.csdn.net/a493823882/article/details/115433888