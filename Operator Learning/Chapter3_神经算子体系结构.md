# 神经算子体系结构

本节综述了文献中使用的主要神经算子架构，包括DeepONets、傅里叶神经算子以及DeepGreen网络。

## Deep Operator Networks (DeepONets)

深度算子网络（DeepONets）是一种用于学习非线性算子并捕捉输入与输出函数之间固有关系的模型。DeepONet的核心理论基础源于通用**算子逼近定理**。该定理可视为神经网络通用逼近算子的无限维类比，其核心思想是：**足够宽的神经网络能够以任意精度逼近任意连续函数。**自提出DeepONet以来，学界围绕该框架在不同场景下的应用展开了深入研究，包括求解伯格方程或对流扩散方程的解算子，以及非线性抛物型偏微分方程的逼近等方向。

DeepONet是一种由分支网络和主干网络组成的两部分深度学习网络。分支网络将算子的输入函数$f$编码为紧凑的固定尺寸潜在向量$b_1(f(x_1), \cdots, f(x_m))，\cdots ，b_p(f(x1), \cdots , f(x_m))$，其中$\{x_i\}_m^{i=1}$表示输入函数的评估传感器点。

## 深度格林网络

### 1. 原文翻译 (3.3 Deep Green networks)

**3.3 深度格林网络 (Deep Green networks)**

Deep Green networks (DGN) 采用了一种不同于 DeepONets 和 FNOs 的方法来逼近 PDE 的解算子 。DGN 不像 DeepONet 那样强制要求积分核具有低秩属性，也不像 FNO 那样要求积分核具有平移不变性，而是**直接在物理空间中学习积分核** 。

假设底层的微分算子是一个线性的边值问题，形式如下：




在适当的正则性假设下（例如一致椭圆性或抛物性），解算子  可以表示为带有格林核 (Green kernel)  的积分算子 ：


(Boullé et al., 2022a) 引入了 **GreenLearning (GL) 网络** 来直接从数据中学习格林核  。GL 的主要思想是将核  参数化为一个神经网络 ，并通过最小化以下相对均方误差损失函数来恢复  的近似值 ：


一旦训练完成，网络  就可以像 FNO 和 DeepONet 一样在域中的任何点进行评估 。这种方法的一个关键优势是它提供了一个**更具可解释性**的模型，因为可以将学习到的核进行可视化和分析，以恢复底层微分算子的性质 。然而，这是以较高的计算复杂度为代价的，因为必须使用求积规则（quadrature rule）精确计算积分操作，这通常需要  次操作，而 FNO 只需要  次操作（其中  是域的空间离散化点数） 。

由于格林函数可能是无界的或奇异的（singular），(Boullé et al., 2022a) 建议使用 **有理神经网络 (rational neural network)** 来逼近格林核 。有理神经网络是一种激活函数为有理函数的神经网络，定义为两个多项式的比值，其系数在网络训练阶段学习 。选择这种架构的动机是，有理网络比标准的 ReLU 网络具有更高的逼近能力，即它们需要指数级更少的层数来以给定精度逼近连续函数，并且可以取任意大的值，这是逼近格林核的一个理想属性 。GL 架构的示意图见图 6 。

当底层的微分算子是**非线性**时，解算子  不能写成带有格林函数的积分算子 。在这种情况下，(Gin et al., 2021) 建议使用双自编码器架构 (dual auto-encoder architecture) —— Deep Green network (DGN) 来学习解算子 。这是一种神经算子架构，它学习一个可逆的坐标变换映射，将非线性边值问题线性化 。得到的线性算子可以用矩阵近似，这代表了一个离散化的格林函数；如果结合 GL 技术，也可以用神经网络表示。这种方法已成功应用于学习非线性立方亥姆霍兹方程的解算子并发现底层的格林函数 。

---

### 2. 深度解析与核心概念

Deep Green Networks (DGN) 的核心在于**回归物理本质**。它不把神经网络仅仅当作一个黑盒函数逼近器，而是利用了偏微分方程（PDE）理论中最优美的结论之一：**格林函数法**。

#### **(1) 什么是格林函数 (Green's Function)?**

在物理直觉上，格林函数  代表了**点源响应**。

* 想象你在一个房间里的  处拍了一下手（由于声音传播，形成一个脉冲源 ）。
* 房间里  处听到的声音就是 。
* 如果你在房间里很多地方同时发出不同的声音（复杂的源项 ），那么你在  处听到的总声音  就是所有这些点源响应的叠加（积分）：


* **DGN 的目标：** 给定一堆输入  和输出 ，让神经网络去“猜”这个房间的声学特性  是什么。

#### **(2) 为什么要用有理神经网络 (Rational Neural Network)?**

这是本节的一个技术亮点。

* **问题：** 许多物理系统的格林函数在源点附近是无限大的（奇异性）。例如，电荷产生的电势 。当距离  时，电势趋向无穷大。
* **ReLU 的局限：** 普通的神经网络使用 ReLU（分段线性函数）。用一堆折线去逼近一个趋向无穷大的  曲线是非常困难且低效的。
* **有理函数的优势：** 有理函数是两个多项式的比值：。
* 如果分母  接近 0，函数值就可以轻松趋向无穷大。
* 这使得它们天生适合模拟具有**奇点**（Singularities）或**极点**（Poles）的物理场。



#### **(3) 线性 vs. 非线性**

* **线性 PDE：** 直接学 。
* **非线性 PDE：** 这是一个难点，因为叠加原理不再适用。DGN 的策略是“**线性化**”。它试图找到一个变换（类似坐标变换），在这个新的空间里，问题变得像线性问题一样，可以用格林函数解决，然后再变换回来。这与算子理论中的 **Koopman 算子**思想有异曲同工之妙。

---

### 3. 具体例子与应用场景

为了更好地理解，我们可以结合你的研究背景（计算机视觉/医学图像）来举例。

#### **例子 1：医学成像中的 CT 重建 (线性场景)**

* **背景：** 在 CT 扫描中，我们测量的是射线穿过人体的衰减（正弦图/Sinogram，即源项 ），我们需要重建人体内部的图像（解 ）。
* **数学模型：** 这本质上是一个逆 Radon 变换，或者可以看作求解一个特定的 PDE。
* **Deep Green Network 的做法：**
* 我们将 （测量数据）和 （真实图像）喂给网络。
* 网络不学习直接从图像到图像的卷积（像 CNN），而是学习一个函数 。
* 这个  代表了：**如果我在正弦图的  位置有一个单位强度的信号，它在重建图像的  位置会产生什么影响？**
* **优势：** 这种方法不仅能重建图像，还能让你看到“物理模型”本身。如果你发现学到的  在某些区域很奇怪，可能意味着扫描仪的物理参数（如几何校正）出了问题。



#### **例子 2：静电场模拟 (奇异性处理)**

* **问题：** 求解泊松方程 。其中  是电荷分布， 是电势。
* **物理事实：** 一个点电荷的电势是 。
* **训练：**
* 输入：随机撒的电荷分布 。
* 输出：对应的电势场 。
* **架构选择：** 如果你用普通的 FNO 或 CNN，在电荷中心附近误差会很大，因为那里电势极高。
* **DGN 的优势：** 它的有理激活函数可以轻易捕捉到  这种形状，只需极少的参数就能极高精度地模拟出电势场，即使在电荷正中心附近也能表现良好。



### 总结

Deep Green Networks 是算子学习中**物理可解释性最强**的方法之一。

* **对比 DeepONet：** DeepONet 把算子看作两组基函数的内积（低秩近似），而 DGN 试图找完整的物理核。
* **对比 FNO：** FNO 假设卷积（平移不变），这对于均匀介质很好，但如果介质不均匀（例如人体组织，不同器官密度不同），平移不变性失效，格林函数  就不再是 。这时 DGN 这种直接学习  的方法更具优势。