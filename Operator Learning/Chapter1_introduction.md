# Introduction of Operator Learning 

## 算子学习的内容

算子学习旨在发现或近似未知算子A，该算子通常表现为与微分方程相关的解算子。从数学角度而言，该问题可定义如下：给定数据对$(f，u)$，其中$f \in U$和$u \in V$来自$d$维空间域$\Omega ⊂ R^d$上的函数空间，以及一个（可能非线性的）算子$A : U → V$，使得$A(f) = u$，目标是找到$A$的近似解 $\hat{A}$ ，使得对于任何新数据$f^{\prime} \in U$，我们有 $\hat{A}（f^{\prime}）≈ A（f^{\prime}）$。换言之，该近似解应同时对训练数据和未见数据准确，从而证明其具有良好泛化能力。

这个问题通常通过将 $\hat{A}$ 表示为神经算子来解决，这是神经网络的推广形式，因为输入和输出是函数而非向量。在将函数在传感器点$x_1 ,... ,x_m \in \Omega$处离散化后，随后用一组参数 $\theta \in R^N$ 对神经算子进行参数化，这些参数可以代表底层神经网络的权重和偏置。然后，通常会构建一个优化问题来寻找最佳参数：$$\min\limits_{\theta \in \mathbb{R}^{N}} \sum\limits_{(f, u) \in datas} L(\hat{A}(f; \theta), u)$$

总体而言，求解算子的恢复具有挑战性，因其通常呈现非线性且高维特性，且可用数据可能稀少或存在噪声。然而，与旨在从解中恢复源项的逆问题不同，正问题通常具有良好的适定性。如后文所述，学习解算子可带来新的见解或应用，这些成果可与逆问题技术形成互补.

## 算子学习 VS. 传统神经网络

传统全连接神经网络，可以写成如下形式：
$$\mathbb{N}(x)=\sigma(A_L(\cdots \sigma(A_1x+b_1)\cdots)+b_L),$$
其中$L\geq 1$是网络层数，$\sigma:\mathbb{R}\leftarrow \mathbb{R}$是激活函数。

而算子学习不同于传统神经网络是离散的向量空间之间的映射，他是函数到函数的映射，可以写成如下形式：
$$u_{i+1}=\sigma(\int_{\Omega_i}K^{i}(x, y)u_i(y)dy+b_i(x)), x\in \Omega_{i+1},$$
这里$\Omega_i \subset \mathbb{R}^{d_i}$是定义域，$K^{i}$是**核函数**。
